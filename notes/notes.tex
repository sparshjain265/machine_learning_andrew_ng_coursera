\documentclass[a4paper, 12pt]{report}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{caption}
\usepackage[normalem]{ulem}
\usepackage{pdfpages}
\usepackage[toc,page]{appendix}

%%% blank footnote
\newcommand\blfootnote[1]{
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}
	\addtocounter{footnote}{-1}
	\endgroup
}
%%%

%%% prevent hyphenation
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000
%%%

%%% minimize
\DeclareMathOperator*{\minimize}{minimize}
%%%


\title{Notes\\
\large Machine Learning by Andrew Ng on Coursera}
\author{Sparsh Jain}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
% \listoffigures
% \listoftables

\chapter{Introduction}
\emph{Machine learning} (task, experience, performance) can be classified into
\emph{Supervised} and \emph{Unsupervised} learning.

\section{Supervised Learning}
Supervised learning can be basically classified into \emph{Regression} and
\emph{Classification} problems.

\subsection{Regression Problem}
Regression problems work loosely on continuous range of outputs.

\subsection{Classification Problems}
Classification problems work loosely on discrete range of outputs.

\section{Unsupervised Learning}
An example is \emph{Clustering Problem}.

% \blfootnote{Check \autoref{lecture1} for more details.}
\blfootnote{Check \href{lecture_pdf/Lecture1.pdf}{Lecture1.pdf} for more details.}

\chapter{Linear Regression with One Variable}

\section{Notations}
\begin{align*}
	m                  & = \text{number of training examples}             \\
	x\text{'s}         & = \text{`input' variables / features}            \\
	y\text{'s}         & = \text{`output' variables / `target' variables} \\
	(x, y)             & = \text{single training example}                 \\
	(x^{(i)}, y^{(i)}) & = i^{th} \  \text{example}                       \\
\end{align*}

\section{Supervised Learning}
We have a data set (\emph{Training Set}).

Training Set $\rightarrow$ Learning Algorithm $\rightarrow$ $h$ (\emph{hypothesis},
a function $X \to Y$)

\subsection*{To Represent \texorpdfstring{$h$}{}}
\begin{equation*}
	h_\theta(x) = \theta_0 + \theta_1x
\end{equation*}

\subsection*{Cost}
\begin{equation*}
	\minimize_{\theta_0,\ \theta_1} \frac{1}{2m}\sum_1^m(h_\theta(x) - y)^2
\end{equation*}
\subsubsection*{Cost Function}
Squared Error Cost Function
\begin{equation*}
	J(\theta_0, \theta_1) = \frac{1}{2m}\sum_1^m(h_\theta(x) - y)^2
\end{equation*}
\subsection*{}
\begin{equation*}
	\minimize_{\theta_0,\ \theta_1} J(\theta_0, \theta_1)
\end{equation*}

\section{Gradient Descent}
Finds local optimum:
\begin{enumerate}
	\item Start with some value
	\item Get closer to optimum
\end{enumerate}

\subsection*{Algorithm}
\begin{equation*}
	\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta) \ \forall j
\end{equation*}

where $\alpha$ = learning rate

\subsubsection*{Important!}
Simultaneous Update!
\begin{align*}
	temp_j   & := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta) \ \forall j \\
	\theta_j & := temp_j \ \forall j
\end{align*}

\section{Gradient Descent for Linear Regression}
Cost function for linear regression is convex!

\emph{Batch Gradient Descent}: Each step of gradient descent uses all training examples.

% \blfootnote{Check \autoref{lecture2} for more details.}
\blfootnote{Check \href{lecture_pdf/Lecture2.pdf}{Lecture2.pdf} for more details.}

\chapter{Linear Algebra}
\section{Matrix}
Rectangular array of numbers:
$$
	\begin{bmatrix}
		1 & 2 & 3 \\
		4 & 5 & 6 \\
	\end{bmatrix}
$$

\paragraph*{Dimension of the matrix:} \#rows x \#cols (2 x 3)

\paragraph*{Elements of the matrix:}
\begin{align*}
	A      & =
	\begin{bmatrix}
		1 & 2 & 3 \\
		4 & 5 & 6 \\
	\end{bmatrix}                                          \\
	A_{ij} & = \text{``$i,j$ entry'' in the $i^{th}$ row, $j^{th}$ col} \\
\end{align*}

\section{Vector}
An $n \times 1$ matrix.
\begin{align*}
	y & =
	\begin{bmatrix}
		1 \\
		2 \\
		3 \\
		4 \\
	\end{bmatrix}   \\
	y_i = i^{th} \text{ element} \\
\end{align*}

\paragraph*{Note:} Uppercase for matrices, lowercase for vectors.

\section{Addition and Scalar Multiplication}
Add/Subtract (element by element) matrices of same dimention only!

Multiply/Divide (all elements) a matrix by scalar!

\section{Matrix Matrix Multiplication}
$m \times n$ matrix multiplied by $n \times o$ matrix gives a $m \times o$ matrix.

\subsection*{Properties}
\begin{enumerate}
	\item Matrix Multiplication is \emph{not} Commutative.
	\item Matrix Multiplication is Associative.
	\item \emph{Identity Matrix ($I$):} $1$'s along diagonal, $0$'s everywhere else in an
	      $n \times n$ matrix. $AI = IA = A$.
\end{enumerate}

\section{Inverse and Transpose}

\subsection*{Inverse}
Only square ($n \times n$) matrices \emph{may} have an inverse.

$$AA^{-1} = A^{-1}A = I$$.

Matrices that don't have an inverse	are \emph{singular} or \emph{degenerate} matrices.

\subsection*{Transpose}
Let $A$ be an $m \times n$ matrix and let $B = A^T$, then
$$ B_{ij} = A_{ji} $$
Example:
\begin{align*}
	A       & =
	\begin{bmatrix}
		1 & 2 & 3 \\
		4 & 5 & 6 \\
	\end{bmatrix} \\
	B = A^T & =
	\begin{bmatrix}
		1 & 4 \\
		2 & 5 \\
		3 & 6 \\
	\end{bmatrix} \\
\end{align*}

% \blfootnote{Check \autoref{lecture3} for more details.}
\blfootnote{Check \href{lecture_pdf/Lecture3.pdf}{Lecture3.pdf} for more details.}



\begin{appendices}
	% \chapter{Lecture 1} \label{lecture1}
	% \includepdf[pages=-, width=0.9\textwidth]{lecture_pdf/Lecture1.pdf}
	% \chapter{Lecture 2} \label{lecture2}
	% \includepdf[pages=-, width=0.9\textwidth]{lecture_pdf/Lecture2.pdf}
	% \chapter{Lecture 3} \label{lecture3}
	% \includepdf[pages=-, width=0.9\textwidth]{lecture_pdf/Lecture3.pdf}
\end{appendices}

\end{document}