\chapter{Regularization}
\section{Problem of Overfitting}
\begin{enumerate}
    \item Underfitting (High Bias)
    \item Right Fit
    \item Overfitting (High Variance)
\end{enumerate}
\paragraph{Overfitting:} If we have too many features, the learned hypothesis may fit
the training set very well
$\left(J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h\theta(x^{(i)}) - y^{(i)})^2 \approx 0\right)$,
but fail to generalize to new examples (predict prices on new examples).

\section{Addressing Overfitting}
Options:
\begin{enumerate}
    \item Reduce the number of features
          \begin{itemize}
              \item Manually select which features to keep
              \item Model selection algorithm (later)
          \end{itemize}
    \item Regularization
          \begin{itemize}
              \item Keep all the features, but reduce the magnitude/values of parameters
                    $\theta_j$
              \item Works well when we have a lot of features, each of which contributes
                    a bit to predicting $y$
          \end{itemize}
\end{enumerate}

\section{Cost Function}
Say our overfitting hypothesis is
$h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$.
Suppose, we penalize and make $\theta_3, \theta_4$ very small
\begin{equation*}
    \minimize_\theta\left(
    \frac{1}{2m}\sum_{i=1}^m\left(
        h_\theta(x^{(i)}) - y^{(i)}
        \right)^2 + 1000\theta_3^2 + 1000\theta_4^2
    \right)
\end{equation*}

\subsection*{Regularization}
Small values for parameters $\theta_0, \theta_1, \dots, \theta_n$.
\begin{itemize}
    \item \emph{Simpler} hypothesis
    \item Less prone to overfitting
\end{itemize}

\paragraph{Which parameters to penalize?}
\begin{itemize}
    \item Features: $x_1, x_2, \dots, x_{100}$
    \item Parameters: $\theta_0, \theta_1, \theta_2, \dots, \theta_{100}$
\end{itemize}
\begin{equation*}
    J(\theta) = \frac{1}{2m}\left[
        \sum_{i=1}^m\left(
        h_\theta(x^{(i)}) - y^{(i)}
        \right)^2 + \lambda\sum_{j=1}^n\theta_j^2
        \right]
\end{equation*}
\paragraph{Note:} The convention, we don't regularize $\theta_0$ but it doesn't
make very much difference.

$\lambda$ here is \emph{regularization parameter}.

What if $\lambda$ is set too high (say $10^{10}$)? Underfitting!

\section{Regularized Linear Regression}
\paragraph{Updated $J(\theta)$:}
\begin{align*}
    J(\theta) & = \frac{1}{2m}\left[
        \sum_{i=1}^m\left(
        h_\theta(x^{(i)}) - y^{(i)}
        \right)^2 + \lambda\sum_{j=1}^n\theta_j^2
        \right]                      \\
    \minimize_\theta J(\theta)       \\
\end{align*}

\paragraph{Gradient Descent:}
\begin{align*}
    \text{Repeat}   & \{                                                             \\
    \theta_0        & :=
    \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\
    \theta_j        & :=
    \theta_j - \alpha \left[
    \frac{1}{m} \sum_{i=1}^m\left(
    (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
    \right) + \frac{\lambda}{m}\theta_j
    \right]         & \forall j \in \{1, 2, \dots, n\}                               \\
    \equiv \theta_j & :=
    \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(
    h_\theta(x^{(i)}) - y^{(i)}
    )x_j^{(i)}                                                                       \\
    \}
\end{align*}

\paragraph{Normal Equation:}
\begin{align*}
    X                   & = \begin{bmatrix}
        \left(x^{(1)}\right)^T \\
        \left(x^{(2)}\right)^T \\
        \vdots                 \\
        \left(x^{(m)}\right)^T \\
    \end{bmatrix} & \in        & \ \R^{m\times(n+1)} \\
    y                   & = \begin{bmatrix}
        y^{(1)} \\
        y^{(2)} \\
        \vdots  \\
        y^{(m)} \\
    \end{bmatrix} & \in        & \ \R^m              \\
    \theta              & = \left(
    X^TX + \lambda \begin{bmatrix}
        0 & 0 & 0 & \dots  & 0 \\
        0 & 1 & 0 & \dots  & 0 \\
        0 & 0 & 1 & \dots  & 0 \\
        0 & 0 & 0 & \ddots & 0 \\
        0 & 0 & 0 & 0      & 1 \\
    \end{bmatrix} \left(\in \R^{(n+1)\times(n+1)}\right)
    \right)^{-1} - X^Ty & \in                          & \ \R^{n+1}                       \\
\end{align*}

\subsubsection*{Non-Invertibility}
Suppose $m \le n$,
$$\theta = \left(X^TX\right)^{-1}X^y$$

If $\lambda > 0$,
$$
    \theta = \left(
    X^TX + \lambda \begin{bmatrix}
        0 &   &   &        &   \\
          & 1 &   &        &   \\
          &   & 1 &        &   \\
          &   &   & \ddots &   \\
          &   &   &        & 1 \\
    \end{bmatrix}
    \right)^{-1} - X^Ty
$$
Not a problem!

\section{Regularized Logistic Regression}
\paragraph{Cost Function:}
\begin{equation*}
    J(\theta) = -\left[
        \frac{1}{m}\sum_{i=1}^m\left(
        y^{(i)}\log h_\theta(x^{(i)}) +
        (1 - y^{(i)})\log(1 - h_\theta(x^{(i)})
        \right)
        \right] + \frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
\end{equation*}

\paragraph{Gradient Descent:}
\begin{align*}
    \text{Repeat}   & \{                                                             \\
    \theta_0        & :=
    \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\
    \theta_j        & :=
    \theta_j - \alpha \left[
    \frac{1}{m} \sum_{i=1}^m\left(
    (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
    \right) + \frac{\lambda}{m}\theta_j
    \right]         & \forall j \in \{1, 2, \dots, n\}                               \\
    \equiv \theta_j & :=
    \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(
    h_\theta(x^{(i)}) - y^{(i)}
    )x_j^{(i)}                                                                       \\
    \}
\end{align*}

\paragraph{Advanced Optimization:}
\begin{minted}{octave}
function [jVal, gradient] = costFunction(theta)
	jVal = [...code to compute J(theta)...];
	gradient = [...code to compute derivative of J(theta)...];
\end{minted}

\blfootnote{Check \href{lecture_pdf/Lecture7.pdf}{Lecture7.pdf} for more details.}