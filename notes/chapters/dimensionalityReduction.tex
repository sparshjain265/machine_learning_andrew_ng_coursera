\chapter{Dimensionality Reduction}
\section{Motivation}
\subsection{Data Compression}
For example reduce data from 2D to 1D $(x^{(i)} \in \R^2 \to z^{(i)} \in \R)$.
This also helps to reduce features along with reducing memory requirement.
More interestingly, it also helps speed up our learning algorithms.

\subsection{Data Visualization}
Helps us to visualize data in a better way since plotting a visualizing
data in smaller dimensions is easier.

\section{Principle Component Analysis}
\subsection{Problem Formulation}
\begin{remark}
	Before applying \emph{PCA} perform mean normalization and feature scaling.
\end{remark}
The goal of \emph{PCA}, if we want to reduce data from 2D to 1D, is, we're going to
find a vector in 2D to project the data minimizing the projection error.

More generally, to reduce from $n$-dimensions to $k$-dimensions, find $k$ vectors
$u^{(1)}$, $u^{(2)}$, \dots, $u^{(k)}$ onto which to project the data, so as to
minimize the projection error.

\begin{remark}
	PCA is not \emph{Linear Regression}
\end{remark}

\section{Algorithm}
\subsection{Data Preprocessing}
Important! Always perform mean normalization and feature scaling.

\subsection{Procedure}
\begin{itemize}
	\item Find component vectors $u^{(i)}$s
	\item Find reduced data values $z^{(i)}$s
\end{itemize}

Reduce data from $n$-dimensions to $k$-dimensions.
\paragraph{Compute \emph{covariance matrix}:}
\begin{align*}
	\Sigma       & = \frac{1}{m} \sum_{i=1}^m(x^{(i)})(x^{(i)})^T \\
	\text{if } X & = \begin{bmatrix}
		-- & (x^{(1)})^T & -- \\
		   & \vdots      &    \\
		-- & (x^{(m)})^T & -- \\
	\end{bmatrix}                    \\
	\Sigma       & = \frac{1}{m} X^TX                             \\
\end{align*}

\paragraph{Compute \emph{eigenvectors} of matrix $\Sigma$:}
SVD stands for \emph{Singular Value Decomposition}
\begin{minted}[escapeinside=||]{octave}
	[U,S,V] = svd(|$\Sigma$|);
\end{minted}
There's also the \mintinline{octave}{eig} function but \mintinline{octave}{svd}
is more numerically stable, though when you apply it on $\Sigma$, it'll give you
the same values and that's because $\Sigma$ satisfies \emph{Symmetric Positive
	Semidefinite}.
\begin{remark}
	$\Sigma \in \R^{n \times n}$
\end{remark}
All we need from \mintinline{octave}{svd} function is the $U \in \R^{n \times n}$ matrix.

The columns of matrix $U$ will be exactly vectors $u^{(i)}$s we want.
\begin{equation*}
	U = \begin{bmatrix}
		\vline  & \vline  &       & \vline  \\
		u^{(1)} & u^{(2)} & \dots & u^{(n)} \\
		\vline  & \vline  &       & \vline  \\
	\end{bmatrix} \in \R^{n \times n}
\end{equation*}
Just take the first $k$ vectors.
\begin{align*}
	U_{reduce} & = \begin{bmatrix}
		\vline  & \vline  &       & \vline  \\
		u^{(1)} & u^{(2)} & \dots & u^{(k)} \\
		\vline  & \vline  &       & \vline  \\
	\end{bmatrix} \in \R^{n \times k} \\
	z          & = \begin{bmatrix}
		\vline  & \vline  &       & \vline  \\
		u^{(1)} & u^{(2)} & \dots & u^{(k)} \\
		\vline  & \vline  &       & \vline  \\
	\end{bmatrix}^Tx \in \R^k         \\
	           & = \begin{bmatrix}
		-- & (u^{(1)})^T & -- \\
		   & \vdots      &    \\
		-- & (u^{(k)})^T & -- \\
	\end{bmatrix}x \in \R^k           \\
\end{align*}

Full procedure in Octave will look like:
\begin{minted}[escapeinside=||]{octave}
	|$\Sigma$| = (1/m)*X'*X;
	[U, S, V] = svd(|$\Sigma$|);
	Ureduce = U(:, 1:k);
	z = Ureduce'*x;
\end{minted}

\subsection{Reconstruction}
\begin{align*}
	z          & = U_{reduce}^Tx \\
	x_{approx} & = U_{reduce}z   \\
\end{align*}

\section{Choosing the number of principle components}
\begin{align*}
	\text{Average squared projection error} & :
	\frac{1}{m} \sum_{i=1}^m \norm{x^{(i)} - x_{approx}^{(i)}}^2 \\
	\text{Total variation in the data}      & :
	\frac{1}{m} \sum_{i=1}^m \norm{x^{(i)}}^2                    \\
\end{align*}

Typically, choose $k$ to be the smallest value so that
\begin{equation*}
	\frac{\frac{1}{m} \sum_{i=1}^m \norm{x^{(i)} - x_{approx}^{(i)}}^2}
	{\frac{1}{m} \sum_{i=1}^m \norm{x^{(i)}}^2} \le 0.01 \ (1\%)
\end{equation*}
\emph{99\% of variance is retained}

Other popular numbers are 95\% or 90\% variance retained.

\subsection{Algorithm}
One way is to try PCA with $k=1, 2, \dots$ and calculate variance retained till
you get 99\% or more. This procedure is horribly inefficient.

From \mintinline[escapeinside=||]{octave}{[U, S, V] = svd(|$\Sigma$|)},
$S \in \R^{n \times n}$ is a diagonal matrix. For, a given $k$:
\begin{equation*}
	\frac{\frac{1}{m} \sum_{i=1}^m \norm{x^{(i)} - x_{approx}^{(i)}}^2}
	{\frac{1}{m} \sum_{i=1}^m \norm{x^{(i)}}^2} = 1 -
	\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}
\end{equation*}
So, we can test:
\begin{equation*}
	\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}} \ge 0.99
\end{equation*}
This is much more efficient!

\section{Applying PCA}
\subsection{Supervised Learning Speedup}
$(x^{(i)}, y^{(i)})$ where say $x^{(i)} \in \R^{10000}$
\begin{enumerate}
	\item Extract Inputs: Unlabelled dataset ($x^{(i)} \in \R^{10000}$)
	\item Apply PCA (say 10x saving) ($x^{(i)} \to z^{(i)} \in \R^{1000}$)
	\item New Training Set: $(z^{(i)}, y^{(i)})$
	\item For test example $x$, map using same PCA to get $z$
\end{enumerate}
\begin{remark}
	Mapping $x^{(i)} \to z^{(i)}$ should be defined by running PCA only on
	the \emph{Training Set}. This mapping can be applied as well to the
	examples in cross-validation and test sets.
\end{remark}

\subsection{Visualization}
Choose $k = 2$ or $k = 3$ coz we know how to plot only in 2 or 3 dimensions.

\subsection{Bad use of PCA: To prevent overfitting}
Use $z^{(i)}$ instead of $x^{(i)}$ to reduce the number of features to $k < n$.
Thus, fewer features, less likely to overfit.

This might work OK, but isn't a good way to address overfitting. Use regularization
instead.

Bad because it throws away information.

\subsection{PCA is sometimes used where it shouldn't be}
Design of ML system:
\begin{enumerate}
	\item Get training set
	\item Run PCA to reduce dimensions (features)
	\item Train
	\item Test
\end{enumerate}

Good question to ask: How about doing the whole thing without PCA?

Before implementing PCA, first try running whatever you want to do with the
original/raw data. Only if that doesn't do what you want, then implement
PCA.

\blfootnote{Check \href{lecture_pdf/Lecture14.pdf}{Lecture14.pdf} for more details.}